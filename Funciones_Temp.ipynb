{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook con todas las funciones desarrolladas en 02.Modelo_Temp_Base para ser importado en los notebooks correspondientes a las diferentes pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_events(data, edificios, n_obs_antes, n_obs_despues, variables):\n",
    "    #DataFrame vacío para la salida\n",
    "    datos_all_rooms = pd.DataFrame()\n",
    "    #import pandas as pd\n",
    "    #Para que no haya problemas de slice y .loc\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    \n",
    "    #Cogemos los datos, y seleccionamos las habitaciones de cada edificio \n",
    "    datos = data\n",
    "    edificio = edificios\n",
    "    mask = datos['block'].isin(edificio)\n",
    "    datos = datos.loc[mask]\n",
    "    rooms = datos['room'].unique()\n",
    "    \n",
    "    #Iteramos sobre cada una de las habitaciones\n",
    "    for room in rooms:\n",
    "        mask = datos['room'].isin([room])\n",
    "        datos_room = datos.loc[mask]\n",
    "        if variables == 1:\n",
    "            datos_room = datos_room[['V2','V4']]\n",
    "        elif variables == 2:\n",
    "            datos_room = datos_room.drop(columns=['dif_cons','cons_total','block','room', 'V12','V26','V5_0','V5_1','V5_2', 'Hora_1', 'Hora_2', 'Hora_3', 'Season_1', 'Season_2', 'Season_3', 'Season_4'],axis=1)\n",
    "        elif variables == 3:\n",
    "            datos_room = datos_room.drop(columns=['dif_cons','cons_total','block','room', 'V12','V26','V5_0','V5_1','V5_2'],axis=1)\n",
    "        else: \n",
    "            print('var_mode error')\n",
    "            break\n",
    "        #Ponemos la columna de V4 como string y buscamos una secuencia de un 1 seguido de n 0's\n",
    "        #El número de 0's lo establece n_obs_despues\n",
    "        text = datos_room.V4.fillna(2).astype(int).astype(str).str.cat()\n",
    "        time = n_obs_despues*10 #Esto nos da el tiempo en min. Ej: n_obs_despues = 6 -> time = 60'\n",
    "\n",
    "        sequence = list(np.repeat(str(0),time/10))\n",
    "        sequence.insert(0,'1'*n_obs_antes)\n",
    "        sequence= ''.join(sequence)\n",
    "        pattern = f'(?:{sequence})'\n",
    "        findings = re.finditer(pattern, text)\n",
    "        events = [(m.start(0)) for m in findings]  \n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        #Para cada una de las secuencias de 1 y 0's, hacemos la selección de las instancias correspondientes\n",
    "        #del conjunto de datos y las añadimos a un df, que iremos completando con cada uno de los eventos\n",
    "        for i in range(len(events)):\n",
    "            ref_inicio = events[i]+n_obs_antes #+1 porque la referencia de inicio es el primer 0 y events empieza en el 1\n",
    "            df1 = datos_room.iloc[range(ref_inicio - n_obs_antes, ref_inicio + n_obs_despues),:]\n",
    "            data = df1\n",
    "            #Una vez que tenemos los eventos quitamos la columna de V4\n",
    "            #data = data.drop(columns='V4')\n",
    "            #n_vars = 1 if type(data) is list else data.shape[1]\n",
    "            df = pd.concat([df,data])\n",
    "        datos_all_rooms = pd.concat([datos_all_rooms, df])\n",
    "    return datos_all_rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(num_features, datos):\n",
    "    data = datos\n",
    "    select = SelectKBest(score_func=f_regression, k=num_features)\n",
    "    z = select.fit_transform(data.iloc[:,1:], data.iloc[:,0]) \n",
    "    filter = select.get_support()\n",
    "    features = np.array(data.iloc[:,1:].columns.values)\n",
    "    features = features[filter].tolist()\n",
    "    features.insert(0, 'V2')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_df(datos, interval):\n",
    "    df=datos\n",
    "    arr_3D = df.values.reshape(-1,interval,df.shape[1])\n",
    "    shuffle_idx = np.random.permutation(arr_3D.shape[0])\n",
    "    arr_3D = arr_3D[shuffle_idx]\n",
    "    shuffled_df = pd.DataFrame(arr_3D.reshape(df.shape[0],df.shape[1]))\n",
    "    return shuffled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, per_train, per_test, interval, n_obs_antes, n_obs_despues):\n",
    "    df=data\n",
    "    train = df.values[0:int((interval*np.ceil(df.shape[0]/interval*per_train)))]\n",
    "    test =  df.values[int((interval*np.ceil(df.shape[0]/interval*per_train))):int((interval*np.ceil(df.shape[0]/interval*(per_train+per_test))))]\n",
    "    val = df.values[int((interval*np.ceil(df.shape[0]/interval*(per_train+per_test)))):]\n",
    "    #Escalado en train y test por separado. Si no se hace aquí ya no se puede porque todo lo que sigue es con arrays 3D\n",
    "    scaler_y = StandardScaler()\n",
    "    train_y = scaler_y.fit_transform(train[:,0].reshape(-1, 1))\n",
    "    val_y = scaler_y.fit_transform(val[:,0].reshape(-1, 1))\n",
    "    test_y = scaler_y.transform(test[:,0].reshape(-1, 1))\n",
    "    scaler_x = StandardScaler()\n",
    "    train_x = scaler_x.fit_transform(train[:,1:]) \n",
    "    val_x = scaler_x.fit_transform(val[:,1:]) \n",
    "    test_x = scaler_x.transform(test[:,1:])\n",
    "    \n",
    "    #Rehacemos los DataFrames con los datos escalados para poder hacer el split\n",
    "    train = pd.concat([pd.DataFrame(train_y),pd.DataFrame(train_x)], axis=1)\n",
    "    test = pd.concat([pd.DataFrame(test_y),pd.DataFrame(test_x)], axis=1)\n",
    "    val = pd.concat([pd.DataFrame(val_y),pd.DataFrame(val_x)], axis=1)\n",
    "\n",
    "    train = np.array(split(train, len(train)/(n_obs_antes+n_obs_despues)))\n",
    "    test = np.array(split(test, len(test)/(n_obs_antes+n_obs_despues)))\n",
    "    val = np.array(split(val, len(val)/(n_obs_antes+n_obs_despues)))\n",
    "    return train, test, val, scaler_x, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(train, n_input, n_out):\n",
    "    # Ponemos los datos en 2D: una entrada para cada muestra y paso de tiempo con las n características\n",
    "    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2])) \n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    # Pasamos por todo el conjunto de datos, de step en step\n",
    "    for _ in range(len(data)):\n",
    "        # Definimos cuando empieza el input, cuando acaba, e igual para el output\n",
    "        # En este caso el output empieza cuando acaba el input\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        # Iteramos mientras haya ejemplos\n",
    "        if out_end <= len(data):\n",
    "            X.append(data[in_start:in_end, :]) # X son todas las variables en t\n",
    "            y.append(data[in_end:out_end, 0])  # y es la temperatura en t+1 (si n_out=1)\n",
    "        # Pasamos al siguiente instante de tiempo\n",
    "        in_start += 1\n",
    "    X = array(X)\n",
    "    y = array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_samples(n_obs_despues, n_obs_antes, train_x, test_x, val_x, train_y, test_y, val_y):\n",
    "    rows_to_keep = n_obs_despues \n",
    "    rows_to_drop = n_obs_antes\n",
    "    period = rows_to_keep + rows_to_drop\n",
    "    \n",
    "    train_sel = (np.arange(len(train_x)) % period) < rows_to_keep\n",
    "    test_sel = (np.arange(len(test_x)) % period) < rows_to_keep\n",
    "    val_sel = (np.arange(len(val_x)) % period) < rows_to_keep\n",
    "    \n",
    "    train_x = train_x[train_sel,:,:]\n",
    "    train_y = train_y[train_sel,:] \n",
    "    test_x = test_x[test_sel,:,:]\n",
    "    test_y = test_y[test_sel,:] \n",
    "    val_x = val_x[val_sel,:,:]\n",
    "    val_y = val_y[val_sel,:] \n",
    "    return train_x, train_y, test_x, test_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp, n_timesteps, n_features, n_outputs):\n",
    "    #Esta estructura hay que retocarla\n",
    "    model = Sequential()\n",
    "    #Numero de nodos en la primera capa \n",
    "    hp_units = hp.Choice('input_unit',[32,64,128])\n",
    "    model.add(LSTM(units = hp_units, input_shape=(n_timesteps, n_features)))\n",
    "    model.add(RepeatVector(n_outputs))   \n",
    "    #Numero de capas ocultas\n",
    "    for i in range(hp.Int('n_layers', 1, 2)):\n",
    "        units_i = hp.Choice(f'dense_{i}_units',[32,64,128])\n",
    "        model.add(LSTM(units = hp_units, return_sequences=True))\n",
    "    #Dropout\n",
    "    hp_dropout = hp.Float('Dropout_rate',min_value=0,max_value=0.4,step=0.1)\n",
    "    model.add(Dropout(hp_dropout))\n",
    "    model.add(TimeDistributed(Dense(units = hp_units)))\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(n_outputs)))\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    model.compile(loss='mae', optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(datos_test_x, datos_test_y, n_predict, n_obs_despues, model, n_timesteps, n_features, n_input):\n",
    "    test_y = datos_test_y\n",
    "    test_x = datos_test_x\n",
    "    #Creamos una lista vacía para las predicciones\n",
    "    predictions = list()\n",
    "    #Ponemos los índices i y j a 0\n",
    "    i=0\n",
    "    #j=0\n",
    "    #Indice para hacer predicciones en un horizonte de tiempo concreto\n",
    "    #Saltamos n filas cada m. Ej: tenemos n_obs_despues=12 pero queremos predecir solo la hora siguiente así\n",
    "    #que predecimos 6 filas, saltamos otras 6 (que serían la 2a hora) y repetimos\n",
    "    n = n_obs_despues - n_predict \n",
    "    m = n_predict   #Salto de n cada m\n",
    "    test_index = [j for k in range(0,len(test_x), n+m) for j in range(k, m+k) if j<len(test_x)]\n",
    "    for i in test_index:\n",
    "        data = test_x[i,:,:]\n",
    "        # Cogemos como input los valores del instante de tiempo anterior al que vamos a predecir\n",
    "        # Recordar que el x lleva un lag de 1 respecto a la y, por eso es el elemento i y no el i-1\n",
    "        # Le damos la forma que el modelo necesita para predecir\n",
    "        input_x = data.reshape((1, n_timesteps, n_features))\n",
    "        \n",
    "        # Prediccion para cada paso de tiempo\n",
    "        yhat = model.predict(input_x, verbose=0)\n",
    "        yhat_sequence = yhat[0] \n",
    "\n",
    "        # Guardamos las predicciones\n",
    "        predictions.append(yhat_sequence)\n",
    "        \n",
    "        # Reescribimos el valor de la temperatura del siguiente paso con la prediccion\n",
    "        # Añadimos una condición más para que en el último paso de cada evento no actualice, ya\n",
    "        # que reescribiría datos de otro caso\n",
    "        if i < len(test_x) and (i not in range(n_predict-1,len(test_x), n_obs_despues)):\n",
    "            try: test_x[i+1,n_input,0] = yhat[0]\n",
    "            except IndexError: continue\n",
    "            #La excepción es porque al llegar al final no hay un i+1 que actualizar y salta un IndexError\n",
    "        #else: break\n",
    "        #j+=1\n",
    "        \n",
    "    predictions = array(predictions)\n",
    "    #Seleccionamos solo los ejemplos del conjunto de test que vamos a predecir\n",
    "    test_y = test_y[test_index,:]\n",
    "    return predictions, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(test_y, prediction):\n",
    "    rmse = np.sqrt(mean_squared_error(test_y,prediction))\n",
    "    cvrmse = rmse/np.mean(test_y)*100\n",
    "    r2 = r2_score(test_y, prediction)\n",
    "    return cvrmse, r2, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
